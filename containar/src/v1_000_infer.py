#!/usr/bin/env python3
"""
LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿Gemma-3 270M æ¨è«–ãƒ†ã‚¹ãƒˆ

å®Ÿè¡Œæ–¹æ³•:
  python src/v1_infer_000.py

å¿…è¦æ¡ä»¶:
  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: ./tmp_docker/models/gemma-3-270m-it
  - LoRAãƒ¢ãƒ‡ãƒ«: ./src/output/v1
  - CPUæ¨è«–ï¼ˆè¨­å®šé€šã‚Šï¼‰
  
å‹•ä½œç¢ºèªæ¸ˆã¿:
  - Axentaxè¨˜æ³•ã®è§£æãƒ»èª¬æ˜
  - éŸ³æ¥½ç†è«–çŸ¥è­˜ã®å¿œç­”
  - æ§‹æ–‡æœ‰åŠ¹æ€§ã®åˆ¤å®š
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")


def main():
    print("ğŸ¯ LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« æœ€çµ‚æ¨è«–ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    base_model_path = "./tmp_docker/models/gemma-3-270m-it"
    lora_model_path = "./src/output/v1"
    
    try:
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        print("Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            dtype=torch.float32,  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚float32
            device_map="cpu",
            trust_remote_code=True
        )
        
        print("Loading LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, lora_model_path)
        model.eval()
        
        print("\nğŸ“ Axentaxæ¨è«–ãƒ†ã‚¹ãƒˆ")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            "USER: ã“ã‚“ã«ã¡ã¯\nASSISTANT:",
            "USER: Axentaxã«ã¤ã„ã¦æ•™ãˆã¦\nASSISTANT:",
            "USER: @@ 120 1/4 { C F G } ã“ã®è¨˜æ³•ã¯ï¼Ÿ\nASSISTANT:"
        ]
        
        for i, prompt in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i} ---")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
            input_length = inputs["input_ids"].shape[1]
            
            print(f"å…¥åŠ›é•·: {input_length} ãƒˆãƒ¼ã‚¯ãƒ³")
            
            # ç”Ÿæˆï¼ˆæœ€ã‚‚å®‰å…¨ãªè¨­å®šï¼‰
            with torch.no_grad():
                try:
                    outputs = model.generate(
                        inputs["input_ids"],
                        max_new_tokens=50,
                        do_sample=False,  # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆ
                        num_beams=1,
                        pad_token_id=tokenizer.eos_token_id,
                    )
                    
                    # ãƒ‡ã‚³ãƒ¼ãƒ‰
                    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
                    print(f"ç”Ÿæˆçµæœ: {response}")
                    
                    if response.strip():
                        print("âœ… æ­£å¸¸ç”Ÿæˆ")
                    else:
                        print("âš ï¸  ç©ºã®å¿œç­”")
                        
                except Exception as e:
                    print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        print(f"LoRAä¿å­˜å ´æ‰€: {lora_model_path}")
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_path}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
        print(f"è¨“ç·´å¯èƒ½: {trainable_params:,}")
        
        print("\nğŸ‰ LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()