#!/usr/bin/env python3
"""
TinyLlama 1.1B å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆGPUç‰ˆï¼‰

æ”¹å–„ç‚¹:
- GPUä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™6GBç¨‹åº¦ï¼‰
- ãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–
- ã‚µãƒ³ãƒ—ãƒ«æ•°èª¿æ•´ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
"""

import json
import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")
from datasets import Dataset
import numpy as np
from typing import List, Dict


def setup_gpu_memory_limit(max_memory_gb=6):
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™"""
    if torch.cuda.is_available():
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™è¨­å®š
        torch.cuda.empty_cache()
        print(f"GPU available: {torch.cuda.get_device_name(0)}")
        print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"Setting memory limit to ~{max_memory_gb}GB")
        return True
    else:
        print("GPU not available, using CPU")
        return False


def load_validation_data(val_path: str, max_samples: int = 50) -> List[Dict]:
    """Validation dataã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼‰"""
    data = []
    with open(val_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            item = json.loads(line.strip())
            data.append(item)
    return data


def calculate_perplexity_batch(model, tokenizer, texts: List[str], batch_size: int = 4) -> float:
    """ãƒãƒƒãƒå‡¦ç†ã§Perplexityã‚’è¨ˆç®—"""
    model.eval()
    device = next(model.parameters()).device
    
    total_loss = 0
    total_tokens = 0
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        
        # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(
            batch_texts, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
            loss = outputs.loss
            
            # ãƒãƒƒãƒå†…ã®æœ‰åŠ¹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
            batch_tokens = attention_mask.sum().item()
            
            total_loss += loss.item() * batch_tokens
            total_tokens += batch_tokens
    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss))
    return perplexity.item()


def evaluate_axentax_batch(model, tokenizer, test_cases: List[Dict], batch_size: int = 2) -> Dict:
    """ãƒãƒƒãƒå‡¦ç†ã§Axentax DSLç†è§£åº¦ã‚’è©•ä¾¡"""
    model.eval()
    device = next(model.parameters()).device
    
    results = {
        "total": len(test_cases),
        "syntax_valid_correct": 0,
        "syntax_invalid_correct": 0,
        "detailed_results": []
    }
    
    for i in range(0, len(test_cases), batch_size):
        batch_cases = test_cases[i:i + batch_size]
        
        # ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompts = []
        expected_validities = []
        
        for case in batch_cases:
            instruction = case["instruction"]
            input_text = case.get("input", "")
            is_valid = case.get("meta", {}).get("valid", True)
            
            if input_text:
                prompt = f"<|user|>\n{instruction}\n{input_text}<|end|>\n<|assistant|>\n"
            else:
                prompt = f"<|user|>\n{instruction}<|end|>\n<|assistant|>\n"
            
            prompts.append(prompt)
            expected_validities.append(is_valid)
        
        # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(
            prompts, 
            return_tensors="pt", 
            truncation=True, 
            max_length=256,
            padding=True
        )
        
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        input_length = input_ids.shape[1]
        
        with torch.no_grad():
            try:
                outputs = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=50,  # çŸ­ç¸®ã§é«˜é€ŸåŒ–
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
                
                # ãƒãƒƒãƒå†…ã®å„å¿œç­”ã‚’å‡¦ç†
                for j, (case, is_valid) in enumerate(zip(batch_cases, expected_validities)):
                    generated_tokens = outputs[j][input_length:]
                    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
                    
                    # è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
                    if is_valid:
                        if "æ§‹æ–‡ã¯æœ‰åŠ¹ã§ã™" in response or "æœ‰åŠ¹ã§ã™" in response:
                            results["syntax_valid_correct"] += 1
                            correct = True
                        else:
                            correct = False
                    else:
                        if "ç„¡åŠ¹ã§ã™" in response or "ã‚¨ãƒ©ãƒ¼" in response:
                            results["syntax_invalid_correct"] += 1
                            correct = True
                        else:
                            correct = False
                    
                    results["detailed_results"].append({
                        "input": case.get("input", ""),
                        "expected_valid": is_valid,
                        "response": response[:100],  # é•·ã•åˆ¶é™
                        "correct": correct
                    })
                    
            except Exception as e:
                for case, is_valid in zip(batch_cases, expected_validities):
                    results["detailed_results"].append({
                        "input": case.get("input", ""),
                        "expected_valid": is_valid,
                        "response": f"ERROR: {e}",
                        "correct": False
                    })
    
    # ç²¾åº¦è¨ˆç®—
    total_valid = sum(1 for case in test_cases if case.get("meta", {}).get("valid", True))
    total_invalid = len(test_cases) - total_valid
    
    valid_accuracy = results["syntax_valid_correct"] / total_valid if total_valid > 0 else 0
    invalid_accuracy = results["syntax_invalid_correct"] / total_invalid if total_invalid > 0 else 0
    overall_accuracy = (results["syntax_valid_correct"] + results["syntax_invalid_correct"]) / len(test_cases)
    
    results["valid_accuracy"] = valid_accuracy
    results["invalid_accuracy"] = invalid_accuracy
    results["overall_accuracy"] = overall_accuracy
    
    return results


def main():
    print("ğŸš€ TinyLlama 1.1B GPUè©•ä¾¡ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™ç‰ˆï¼‰")
    print("=" * 60)
    
    # GPUè¨­å®š
    use_gpu = setup_gpu_memory_limit(6)
    device = "cuda" if use_gpu else "cpu"
    dtype = torch.float16 if use_gpu else torch.float32
    
    # ãƒ‘ã‚¹è¨­å®š
    base_model_path = "./tmp_docker/models/tinyllama-1.1b-chat"
    lora_model_path = "./src/output/v2"
    val_path = "./tmp_docker/dataset/validation.jsonl"
    
    try:
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("Loading models...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            dtype=dtype,
            device_map="auto" if use_gpu else "cpu",
            trust_remote_code=True,
            torch_dtype=dtype
        )
        
        trained_model = PeftModel.from_pretrained(base_model, lora_model_path)
        trained_model.eval()
        
        print(f"Model loaded on: {next(trained_model.parameters()).device}")
        
        # Validation dataèª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼‰
        print("Loading validation data...")
        val_data = load_validation_data(val_path, max_samples=30)  # 30ä»¶ã«åˆ¶é™
        print(f"Loaded {len(val_data)} validation samples")
        
        # 1. Perplexityè©•ä¾¡ï¼ˆ15ä»¶ã€ãƒãƒƒãƒã‚µã‚¤ã‚º3ï¼‰
        print("\nğŸ“Š 1. Perplexityè©•ä¾¡")
        sample_texts = []
        for item in val_data[:15]:  # 15ä»¶ã«åˆ¶é™
            input_text = item.get('input', '').strip()
            if input_text:
                text = f"<|user|>\n{item['instruction']}\n{input_text}<|end|>\n<|assistant|>\n{item['output']}<|end|>"
            else:
                text = f"<|user|>\n{item['instruction']}<|end|>\n<|assistant|>\n{item['output']}<|end|>"
            sample_texts.append(text)
        
        perplexity = calculate_perplexity_batch(trained_model, tokenizer, sample_texts, batch_size=3)
        print(f"Perplexity: {perplexity:.2f}")
        
        # 2. Axentax DSLç†è§£åº¦è©•ä¾¡ï¼ˆ20ä»¶ã€ãƒãƒƒãƒã‚µã‚¤ã‚º2ï¼‰
        print("\nğŸµ 2. Axentax DSLç†è§£åº¦è©•ä¾¡")
        axentax_results = evaluate_axentax_batch(trained_model, tokenizer, val_data[:20], batch_size=2)
        print(f"Overall Accuracy: {axentax_results['overall_accuracy']:.3f}")
        print(f"Valid Syntax Accuracy: {axentax_results['valid_accuracy']:.3f}")
        print(f"Invalid Syntax Accuracy: {axentax_results['invalid_accuracy']:.3f}")
        
        # 3. ç°¡å˜ãªè³ªå•å¿œç­”ãƒ†ã‚¹ãƒˆï¼ˆGPUç‰ˆã¯çœç•¥ï¼‰
        print("\nğŸ’¬ 3. ä¸€èˆ¬QAãƒ†ã‚¹ãƒˆã¯çœç•¥ï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰")
        
        # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        if use_gpu:
            memory_used = torch.cuda.max_memory_allocated() / 1e9
            print(f"\nğŸ”§ GPUæœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.2f}GB")
        
        # çµæœä¿å­˜
        evaluation_report = {
            "perplexity": perplexity,
            "axentax_evaluation": {
                "overall_accuracy": axentax_results['overall_accuracy'],
                "valid_accuracy": axentax_results['valid_accuracy'],
                "invalid_accuracy": axentax_results['invalid_accuracy'],
                "total_samples": len(val_data[:20])
            },
            "evaluation_config": {
                "device": str(device),
                "dtype": str(dtype),
                "perplexity_samples": 15,
                "axentax_samples": 20,
                "max_memory_used_gb": torch.cuda.max_memory_allocated() / 1e9 if use_gpu else 0
            }
        }
        
        os.makedirs("./src/output", exist_ok=True)
        with open("./src/output/evaluation_report_gpu.json", "w", encoding="utf-8") as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
        
        print("ğŸ‰ GPUè©•ä¾¡å®Œäº†! ãƒ¬ãƒãƒ¼ãƒˆ: ./src/output/evaluation_report_gpu.json")
        
    except Exception as e:
        print(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if use_gpu:
            torch.cuda.empty_cache()
            print("GPU cache cleared")


if __name__ == "__main__":
    main()