#!/usr/bin/env python3
"""
TinyLlama 1.1B å­¦ç¿’æ¸ˆã¿LoRAãƒ¢ãƒ‡ãƒ« æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰

ä¿®æ­£å†…å®¹:
- pad_tokenè¨­å®šè¿½åŠ 
- attention_maskå¯¾å¿œ
- æ¨è«–æ™‚ã®LoRAçŠ¶æ…‹ç¢ºèª
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")


def main():
    print("ğŸ¯ TinyLlama 1.1B å­¦ç¿’æ¸ˆã¿LoRAãƒ¢ãƒ‡ãƒ« æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("=" * 60)
    
    base_model_path = "./tmp_docker/models/tinyllama-1.1b-chat"
    lora_model_path = "./src/output/v2"
    
    try:
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print(f"pad_token: {tokenizer.pad_token}")
        print(f"eos_token: {tokenizer.eos_token}")
        
        print("Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True
        )
        
        print("Loading LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, lora_model_path)
        
        # LoRAçŠ¶æ…‹ç¢ºèª
        print(f"LoRA merged: {model.peft_config}")
        print(f"Active adapters: {model.active_adapters}")
        
        model.eval()
        
        print("\nğŸ“ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¨è«–ãƒ†ã‚¹ãƒˆ")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            "<|user|>\nã“ã‚“ã«ã¡ã¯ã€‚ã‚ãªãŸã¯ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ<|end|>\n<|assistant|>\n",
            "<|user|>\nç°¡å˜ãªè³ªå•ã§ã™ã€‚1+1ã¯ï¼Ÿ<|end|>\n<|assistant|>\n",
            "<|user|>\néŸ³æ¥½ã«ã¤ã„ã¦ä½•ã‹æ•™ãˆã¦<|end|>\n<|assistant|>\n"
        ]
        
        for i, prompt in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i} ---")
            print(f"å…¥åŠ›: {prompt}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=128,
                padding=True
            )
            input_length = inputs["input_ids"].shape[1]
            
            print(f"å…¥åŠ›é•·: {input_length} ãƒˆãƒ¼ã‚¯ãƒ³")
            print(f"attention_mask shape: {inputs['attention_mask'].shape}")
            
            # ç”Ÿæˆ
            with torch.no_grad():
                try:
                    outputs = model.generate(
                        inputs["input_ids"],
                        attention_mask=inputs["attention_mask"],
                        max_new_tokens=50,
                        do_sample=False,  # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆã§å®‰å®šåŒ–
                        num_beams=1,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    
                    # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ãƒ‡ã‚³ãƒ¼ãƒ‰
                    generated_tokens = outputs[0][input_length:]
                    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    
                    print(f"ç”Ÿæˆçµæœ: '{response}'")
                    
                    if response.strip():
                        print("âœ… æ­£å¸¸ç”Ÿæˆ")
                    else:
                        print("âš ï¸  ç©ºã®å¿œç­”")
                        
                except Exception as e:
                    print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()
        
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_path}")
        print(f"LoRAã‚¢ãƒ€ãƒ—ã‚¿: {lora_model_path}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèªï¼ˆæ­£ã—ã„æ–¹æ³•ï¼‰
        total_params = sum(p.numel() for p in base_model.parameters())
        
        # LoRA adapter parameters
        lora_params = 0
        for name, param in model.named_parameters():
            if "lora" in name.lower():
                lora_params += param.numel()
        
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
        print(f"LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {lora_params:,}")
        
        print("\nğŸ‰ ä¿®æ­£ç‰ˆæ¨è«–ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()